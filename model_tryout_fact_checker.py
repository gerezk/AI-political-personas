# -*- coding: utf-8 -*-
"""model_tryout_fact_checker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11vfadQ_LCWZa_7_MLlQxB3hCvgvVP2bc

The files `democrat_....mf` and `republican_..mf` are used to build/finetune the ollama models.
I adjusted the .mf files, so the models are restricted to a 150 token output, using `PARAMETER num_predict 150`.

You can either build the models on your own on your local machine or just pull it from the ollama cloud.

# Download the models

- Republican model at: https://ollama.com/nadinekitzwoegerer/rep-model:v2

- Democrat model at: https://ollama.com/nadinekitzwoegerer/dem-model:v2

# Example usage of the models
"""

#%pip install ollama tqdm
import ollama
import json
from tqdm import tqdm

# ANSI color codes
class Colors:
    BLUE = '\033[94m'      # Democrat
    RED = '\033[91m'       # Republican
    GREEN = '\033[92m'     # Fact-checker / Correct
    YELLOW = '\033[93m'    # Moderator / Warning
    MAGENTA = '\033[95m'   # User question
    CYAN = '\033[96m'      # Info
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    RESET = '\033[0m'

def print_colored(text: str, color: str, bold: bool = False):
    """Print text with color."""
    prefix = Colors.BOLD if bold else ''
    print(f"{prefix}{color}{text}{Colors.RESET}")

democrat_model = 'nadinekitzwoegerer/dem-model:v2'
republican_model = 'nadinekitzwoegerer/rep-model:v2'

question1 = "If you believe life begins at conception, how can you justify letting different states have different laws? Isn't a human life just as valuable in California as it is in Alabama?"

messages = [
    {
        'role': 'user',
        'content': question1
    }
]

response = ollama.chat(
    model=democrat_model,
    messages=messages,
    #format='json'
)

print(response['message']['content'])


print(json.dumps(response.model_dump(), indent=4))

response = ollama.chat(model=republican_model, messages=messages)
print(response['message']['content'])

print(json.dumps(response.model_dump(), indent=4))

question2 = "What is your opinion on gun usage? Should it be restricted? How would you implement such a restriction or ban?"

messages = [
    {
        'role': 'user',
        'content': question2
    }
]

response = ollama.chat(model=democrat_model, messages=messages)
print(response['message']['content'])

response = ollama.chat(model=republican_model, messages=messages)
print(response['message']['content'])

"""# Fact-checker model (built like the personas)
This section builds a local fact-checker Ollama model from a Modelfile and runs an automated per-turn pipeline: DEM + REP -> fact-check.

## 1) Build (or pull) the fact-checker model
We'll create a new local Ollama model based on **HammerAI/mistral-nemo-uncensored:latest**.

> Note: This model is heavy. Expect slow inference on CPU and/or low-RAM machines.
"""

import os
import textwrap

FACTCHECK_BASE = 'HammerAI/mistral-nemo-uncensored:latest'
FACTCHECK_MODEL = 'group5/fact-checker-mistral:v1'

modelfile_path = 'FactCheckerPersona_mistral.mf'

modelfile = textwrap.dedent('''FROM {base}

SYSTEM """
You are a FACT-CHECKER moderating a political debate.
You are neutral, careful, and evidence-focused.

Your job for each user turn:
1) Extract empirically checkable factual claims from each politician answer (DEM and REP).
2) Assess each claim. If you cannot verify confidently without provided sources, label it UNVERIFIABLE_WITHOUT_SOURCES. Do NOT guess.
3) Produce one combined moderator message summarizing key issues briefly.

Do NOT evaluate opinions, values, or policy proposals ("should", "I will"). Only extract factual claims.
Split compound claims into atomic claims.

When the user asks to fact-check a turn, output VALID JSON ONLY in this schema:
{{
  "results": [
    {{
      "origin": "DEM|REP",
      "claims_checked": [
        {{
          "claim_text": "... ",
          "verdict": "CORRECT|INCORRECT|MIXED|UNVERIFIABLE_WITHOUT_SOURCES",
          "why_short": "... (max 2 sentences)",
          "what_to_verify": ["... ","... "]
        }}
      ]
    }}
  ],
  "moderator_message": "..."
}}
"""
''').format(base=FACTCHECK_BASE)

with open(modelfile_path, 'w', encoding='utf-8') as f:
    f.write(modelfile)

print('Wrote', modelfile_path)
print('Base:', FACTCHECK_BASE)
print('Target model:', FACTCHECK_MODEL)

# Build the model (run this in a terminal if Jupyter blocks shell commands):
# !ollama pull HammerAI/mistral-nemo-uncensored:latest
# !ollama create group5/fact-checker-mistral:v1 -f FactCheckerPersona_mistral.mf

# In Jupyter, you can try:
# !ollama pull {FACTCHECK_BASE}
# !ollama create {FACTCHECK_MODEL} -f {modelfile_path}

"""## 2) A small helper: heartbeat timer (no timeout)
This prints a message every few seconds so you can see the call is still running.

"""

import time
import threading

# If you didn't install the package yet:
# %pip install ollama
import ollama


def chat_with_heartbeat(model: str, messages, *, every_s: int = 5, stream: bool = False, **kwargs):
    """Run ollama.chat with a heartbeat timer so it doesn't look stuck.
    - If stream=True, prints tokens as they arrive (best feedback).
    - If stream=False, prints elapsed time every `every_s` seconds until the call returns.
    """
    start = time.time()
    stop_event = threading.Event()

    def heartbeat():
        while not stop_event.wait(every_s):
            elapsed = int(time.time() - start)
            print(f'... still running ({elapsed}s elapsed)')

    t = threading.Thread(target=heartbeat, daemon=True)
    t.start()

    try:
        if stream:
            parts = []
            for chunk in ollama.chat(model=model, messages=messages, stream=True, **kwargs):
                txt = chunk.get('message', {}).get('content', '')
                if txt:
                    print(txt, end='', flush=True)
                    parts.append(txt)
            print()
            return {'message': {'content': ''.join(parts)}}
        else:
            resp = ollama.chat(model=model, messages=messages, **kwargs)
            return resp
    finally:
        stop_event.set()

"""## 3) Automated turn: DEM + REP -> fact-check -> moderator
This cell runs the pipeline for a single user turn.

"""

import json

DEMOCRAT_MODEL = 'nadinekitzwoegerer/dem-model:v2'
REPUBLICAN_MODEL = 'nadinekitzwoegerer/rep-model:v2'
FACTCHECK_MODEL = 'group5/fact-checker-mistral:v1'


def run_turn(user_text: str, *, stream: bool = False):
    steps = [
        ("Democrat response", DEMOCRAT_MODEL, Colors.BLUE),
        ("Republican response", REPUBLICAN_MODEL, Colors.RED),
        ("Fact-checking", FACTCHECK_MODEL, Colors.GREEN),
    ]

    pbar = tqdm(total=len(steps), desc="Processing", bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')

    # 1) Get Democrat answer
    pbar.set_description(f"{Colors.BLUE}Getting Democrat response{Colors.RESET}")
    dem_resp = chat_with_heartbeat(DEMOCRAT_MODEL, [{'role':'user','content': user_text}], stream=stream)
    dem_text = dem_resp['message']['content']
    pbar.update(1)

    # 2) Get Republican answer
    pbar.set_description(f"{Colors.RED}Getting Republican response{Colors.RESET}")
    rep_resp = chat_with_heartbeat(REPUBLICAN_MODEL, [{'role':'user','content': user_text}], stream=stream)
    rep_text = rep_resp['message']['content']
    pbar.update(1)

    # 3) Fact-check
    pbar.set_description(f"{Colors.GREEN}Fact-checking responses{Colors.RESET}")
    fc_prompt = f"""FACTCHECK_TURN
User: {user_text}

DEM answer:
{dem_text}

REP answer:
{rep_text}

Return JSON with fields: results (per origin) and moderator_message.
Only include factual, checkable claims. Exclude policy proposals/opinions.
If unverifiable without sources, use UNVERIFIABLE_WITHOUT_SOURCES and list what_to_verify.
"""

    fc_resp = chat_with_heartbeat(
        FACTCHECK_MODEL,
        [{'role':'user','content': fc_prompt}],
        stream=stream,
        format='json'
    )

    fc_obj = json.loads(fc_resp['message']['content'])
    pbar.update(1)
    pbar.close()

    return {
        'user_text': user_text,
        'answers': [
            {'origin': 'DEM', 'text': dem_text},
            {'origin': 'REP', 'text': rep_text}
        ],
        'fact_check': fc_obj
    }

"""## 4) Try it
Set `stream=True` if you want token-by-token output (best for seeing progress).

"""

import json

question = "What is your opinion on gun usage? Should it be restricted? How would you implement such a restriction or ban?"

print_colored(f"\n{'='*60}", Colors.MAGENTA, bold=True)
print_colored(f"QUESTION: {question}", Colors.MAGENTA, bold=True)
print_colored(f"{'='*60}\n", Colors.MAGENTA, bold=True)

turn_out = run_turn(question, stream=False)

print_colored(f"\n{'â”€'*60}", Colors.BLUE)
print_colored("ðŸ”µ DEMOCRAT ANSWER", Colors.BLUE, bold=True)
print_colored(f"{'â”€'*60}", Colors.BLUE)
print_colored(turn_out['answers'][0]['text'], Colors.BLUE)

print_colored(f"\n{'â”€'*60}", Colors.RED)
print_colored("ðŸ”´ REPUBLICAN ANSWER", Colors.RED, bold=True)
print_colored(f"{'â”€'*60}", Colors.RED)
print_colored(turn_out['answers'][1]['text'], Colors.RED)

print_colored(f"\n{'â”€'*60}", Colors.GREEN)
print_colored("âœ“ FACT-CHECK RESULTS", Colors.GREEN, bold=True)
print_colored(f"{'â”€'*60}", Colors.GREEN)
print_colored(json.dumps(turn_out['fact_check'], indent=2, ensure_ascii=False), Colors.GREEN)

print_colored(f"\n{'â”€'*60}", Colors.YELLOW)
print_colored("ðŸ“¢ MODERATOR MESSAGE", Colors.YELLOW, bold=True)
print_colored(f"{'â”€'*60}", Colors.YELLOW)
print_colored(turn_out['fact_check'].get('moderator_message',''), Colors.YELLOW)